{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description：\n",
    "这个笔记本要做一个GBDT+LR的demon， 基于kaggle上的一个比赛数据集, 下载链接：[http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/](http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/) 数据集介绍：\n",
    "这是criteo-Display Advertising Challenge比赛的部分数据集， 里面有train.csv和test.csv两个文件：\n",
    "* train.csv： 训练集由Criteo 7天内的部分流量组成。每一行对应一个由Criteo提供的显示广告。为了减少数据集的大小，正(点击)和负(未点击)的例子都以不同的比例进行了抽样。示例是按时间顺序排列的\n",
    "* test.csv: 测试集的计算方法与训练集相同，只是针对训练期之后一天的事件\n",
    "\n",
    "字段说明：\n",
    "* Label： 目标变量， 0表示未点击， 1表示点击\n",
    "* l1-l13: 13列的数值特征， 大部分是计数特征\n",
    "* C1-C26: 26列分类特征， 为了达到匿名的目的， 这些特征的值离散成了32位的数据表示\n",
    "\n",
    "这个比赛的任务就是：开发预测广告点击率(CTR)的模型。给定一个用户和他正在访问的页面，预测他点击给定广告的概率是多少？比赛的地址链接：[https://www.kaggle.com/c/criteo-display-ad-challenge/overview](https://www.kaggle.com/c/criteo-display-ad-challenge/overview)\n",
    "<br><br>\n",
    "下面基于GBDT+LR模型完后这个任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:02:40.455261Z",
     "start_time": "2020-09-10T12:02:39.318459Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"导入包\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import gc\n",
    "from scipy import sparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据导入与简单处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:02:41.574600Z",
     "start_time": "2020-09-10T12:02:41.533663Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"数据读取与预处理\"\"\"\n",
    "\n",
    "# 数据读取\n",
    "path = 'data/'\n",
    "df_train = pd.read_csv(path + 'train.csv')\n",
    "df_test = pd.read_csv(path + 'test.csv')\n",
    "\n",
    "# 简单的数据预处理\n",
    "# 去掉id列， 把测试集和训练集合并， 填充缺失值\n",
    "df_train.drop(['Id'], axis=1, inplace=True)\n",
    "df_test.drop(['Id'], axis=1, inplace=True)\n",
    "\n",
    "df_test['Label'] = -1\n",
    "\n",
    "data = pd.concat([df_train, df_test])\n",
    "data.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:02:42.184548Z",
     "start_time": "2020-09-10T12:02:42.180557Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"下面把特征列分开处理\"\"\"\n",
    "continuous_fea = ['I'+str(i+1) for i in range(13)]\n",
    "category_fea = ['C'+str(i+1) for i in range(26)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建模\n",
    "下面训练三个模型对数据进行预测， 分别是LR模型， GBDT模型和两者的组合模型， 然后分别观察它们的预测效果， 对于不同的模型， 特征会有不同的处理方式如下：\n",
    "1. 逻辑回归模型： 连续特征要归一化处理， 离散特征需要one-hot处理\n",
    "2. GBDT模型： 树模型连续特征不需要归一化处理， 但是离散特征需要one-hot处理\n",
    "3. LR+GBDT模型： 由于LR使用的特征是GBDT的输出， 原数据依然是GBDT进行处理交叉， 所以只需要离散特征one-hot处理\n",
    "\n",
    "下面就通过函数的方式建立三个模型， 并进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:02:44.358065Z",
     "start_time": "2020-09-10T12:02:44.350087Z"
    }
   },
   "outputs": [],
   "source": [
    "def lr_model(data, category_fea, continuous_fea):\n",
    "    # 连续特征归一化\n",
    "    scaler = MinMaxScaler()\n",
    "    for col in continuous_fea:\n",
    "        data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n",
    "    \n",
    "    # 离散特征one-hot编码\n",
    "    for col in category_fea:\n",
    "        onehot_feats = pd.get_dummies(data[col], prefix=col)\n",
    "        data.drop([col], axis=1, inplace=True)\n",
    "        data = pd.concat([data, onehot_feats], axis=1)\n",
    "    \n",
    "    # 把训练集和测试集分开\n",
    "    train = data[data['Label'] != -1]\n",
    "    target = train.pop('Label')\n",
    "    test = data[data['Label'] == -1]\n",
    "    test.drop(['Label'], axis=1, inplace=True)\n",
    "    \n",
    "    # 划分数据集\n",
    "    x_train, x_val, y_train, y_val = train_test_split(train, target, test_size=0.2, random_state=2020)\n",
    "    \n",
    "    # 建立模型\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    tr_logloss = log_loss(y_train, lr.predict_proba(x_train)[:, 1])   # −(ylog(p)+(1−y)log(1−p)) log_loss\n",
    "    val_logloss = log_loss(y_val, lr.predict_proba(x_val)[:, 1])\n",
    "    print('tr_logloss: ', tr_logloss)\n",
    "    print('val_logloss: ', val_logloss)\n",
    "    \n",
    "    # 模型预测\n",
    "    y_pred = lr.predict_proba(test)[:, 1]  # predict_proba 返回n行k列的矩阵，第i行第j列上的数值是模型预测第i个预测样本为某个标签的概率, 这里的1表示点击的概率\n",
    "    print('predict: ', y_pred[:10]) # 这里看前10个， 预测为点击的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:02:48.767635Z",
     "start_time": "2020-09-10T12:02:45.774402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_logloss:  0.12438102782711172\n",
      "val_logloss:  0.44371852425737474\n",
      "predict:  [0.45032286 0.80663244 0.1777058  0.02125111 0.1397029  0.46540529\n",
      " 0.4349664  0.07067935 0.07172526 0.27862524]\n"
     ]
    }
   ],
   "source": [
    "# 训练和预测\n",
    "lr_model(data.copy(), category_fea, continuous_fea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:03:01.725802Z",
     "start_time": "2020-09-10T12:03:01.717823Z"
    }
   },
   "outputs": [],
   "source": [
    "def gbdt_model(data, category_fea, continuous_fea):\n",
    "    \n",
    "    # 离散特征one-hot编码\n",
    "    for col in category_fea:\n",
    "        onehot_feats = pd.get_dummies(data[col], prefix=col)\n",
    "        data.drop([col], axis=1, inplace=True)\n",
    "        data = pd.concat([data, onehot_feats], axis=1)\n",
    "    \n",
    "    # 训练集和测试集分开\n",
    "    train = data[data['Label'] != -1]\n",
    "    target = train.pop('Label')\n",
    "    test = data[data['Label'] == -1]\n",
    "    test.drop(['Label'], axis=1, inplace=True)\n",
    "    \n",
    "    # 划分数据集\n",
    "    x_train, x_val, y_train, y_val = train_test_split(train, target, test_size=0.2, random_state=2020)\n",
    "    \n",
    "    # 建模\n",
    "    gbm = lgb.LGBMClassifier(boosting_type='gbdt',  # 这里用gbdt\n",
    "                             objective='binary', \n",
    "                             subsample=0.8,\n",
    "                             min_child_weight=0.5, \n",
    "                             colsample_bytree=0.7,\n",
    "                             num_leaves=100,\n",
    "                             max_depth=12,\n",
    "                             learning_rate=0.01,\n",
    "                             n_estimators=10000\n",
    "                            )\n",
    "    gbm.fit(x_train, y_train, \n",
    "            eval_set=[(x_train, y_train), (x_val, y_val)], \n",
    "            eval_names=['train', 'val'],\n",
    "            eval_metric='binary_logloss',\n",
    "            early_stopping_rounds=100,\n",
    "           )\n",
    "    \n",
    "    tr_logloss = log_loss(y_train, gbm.predict_proba(x_train)[:, 1])   # −(ylog(p)+(1−y)log(1−p)) log_loss\n",
    "    val_logloss = log_loss(y_val, gbm.predict_proba(x_val)[:, 1])\n",
    "    print('tr_logloss: ', tr_logloss)\n",
    "    print('val_logloss: ', val_logloss)\n",
    "    \n",
    "    # 模型预测\n",
    "    y_pred = gbm.predict_proba(test)[:, 1]  # predict_proba 返回n行k列的矩阵，第i行第j列上的数值是模型预测第i个预测样本为某个标签的概率, 这里的1表示点击的概率\n",
    "    print('predict: ', y_pred[:10]) # 这里看前10个， 预测为点击的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:03:05.585613Z",
     "start_time": "2020-09-10T12:03:02.318820Z"
    }
   },
   "outputs": [],
   "source": [
    "# 模型训练和预测\n",
    "try:\n",
    "    gbdt_model(data.copy(), category_fea, continuous_fea)\n",
    "except TypeError:\n",
    "    # early_stopping_rounds is not supported, use callbacks instead\n",
    "    gbdt_model = lambda x,y,z: lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                                                 objective='binary',\n",
    "                                                 subsample=0.8, \n",
    "                                                 min_child_weight=0.5,\n",
    "                                                 colsample_bytree=0.7,\n",
    "                                                 num_leaves=100,\n",
    "                                                 max_depth=12,\n",
    "                                                 learning_rate=0.01,\n",
    "                                                 n_estimators=10000,\n",
    "                                                 callbacks=[lgb.early_stopping(100)]\n",
    "                                                ).fit(x_train, y_train,\n",
    "                                                     eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "                                                     eval_names=['train', 'val'],\n",
    "                                                     eval_metric='binary_logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR + GBDT建模\n",
    "下面就是把上面两个模型进行组合， GBDT负责对各个特征进行交叉和组合， 把原始特征向量转换为新的离散型特征向量， 然后在使用逻辑回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:08:05.780599Z",
     "start_time": "2020-09-10T12:08:05.765639Z"
    }
   },
   "outputs": [],
   "source": [
    "def gbdt_lr_model(data, category_feature, continuous_feature): # 0.43616\n",
    "    # 离散特征one-hot编码\n",
    "    for col in category_feature:\n",
    "        onehot_feats = pd.get_dummies(data[col], prefix = col)\n",
    "        data.drop([col], axis = 1, inplace = True)\n",
    "        data = pd.concat([data, onehot_feats], axis = 1)\n",
    "\n",
    "    train = data[data['Label'] != -1]\n",
    "    target = train.pop('Label')\n",
    "    test = data[data['Label'] == -1]\n",
    "    test.drop(['Label'], axis = 1, inplace = True)\n",
    "\n",
    "    # 划分数据集\n",
    "    x_train, x_val, y_train, y_val = train_test_split(train, target, test_size = 0.2, random_state = 2020)\n",
    "\n",
    "    gbm = lgb.LGBMClassifier(objective='binary',\n",
    "                            subsample= 0.8,\n",
    "                            min_child_weight= 0.5,\n",
    "                            colsample_bytree= 0.7,\n",
    "                            num_leaves=100,\n",
    "                            max_depth = 12,\n",
    "                            learning_rate=0.01,\n",
    "                            n_estimators=1000,\n",
    "                            )\n",
    "\n",
    "    gbm.fit(x_train, y_train,\n",
    "            eval_set = [(x_train, y_train), (x_val, y_val)],\n",
    "            eval_names = ['train', 'val'],\n",
    "            eval_metric = 'binary_logloss',\n",
    "            early_stopping_rounds = 100,\n",
    "            )\n",
    "    \n",
    "    model = gbm.booster_\n",
    "\n",
    "    gbdt_feats_train = model.predict(train, pred_leaf = True)\n",
    "    gbdt_feats_test = model.predict(test, pred_leaf = True)\n",
    "    gbdt_feats_name = ['gbdt_leaf_' + str(i) for i in range(gbdt_feats_train.shape[1])]\n",
    "    df_train_gbdt_feats = pd.DataFrame(gbdt_feats_train, columns = gbdt_feats_name) \n",
    "    df_test_gbdt_feats = pd.DataFrame(gbdt_feats_test, columns = gbdt_feats_name)\n",
    "\n",
    "    train = pd.concat([train, df_train_gbdt_feats], axis = 1)\n",
    "    test = pd.concat([test, df_test_gbdt_feats], axis = 1)\n",
    "    train_len = train.shape[0]\n",
    "    data = pd.concat([train, test])\n",
    "    del train\n",
    "    del test\n",
    "    gc.collect()\n",
    "\n",
    "    # # 连续特征归一化\n",
    "    scaler = MinMaxScaler()\n",
    "    for col in continuous_feature:\n",
    "        data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n",
    "\n",
    "    for col in gbdt_feats_name:\n",
    "        onehot_feats = pd.get_dummies(data[col], prefix = col)\n",
    "        data.drop([col], axis = 1, inplace = True)\n",
    "        data = pd.concat([data, onehot_feats], axis = 1)\n",
    "\n",
    "    train = data[: train_len]\n",
    "    test = data[train_len:]\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(train, target, test_size = 0.3, random_state = 2018)\n",
    "\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    tr_logloss = log_loss(y_train, lr.predict_proba(x_train)[:, 1])\n",
    "    print('tr-logloss: ', tr_logloss)\n",
    "    val_logloss = log_loss(y_val, lr.predict_proba(x_val)[:, 1])\n",
    "    print('val-logloss: ', val_logloss)\n",
    "    y_pred = lr.predict_proba(test)[:, 1]\n",
    "    print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:08:32.439103Z",
     "start_time": "2020-09-10T12:08:07.548420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: early_stopping_rounds is not supported in this version of LightGBM\n",
      "Try using callbacks instead:\n",
      "callbacks=[lgb.early_stopping(100)]\n"
     ]
    }
   ],
   "source": [
    "# 训练和预测\n",
    "try:\n",
    "    gbdt_lr_model(data.copy(), category_fea, continuous_fea)\n",
    "except TypeError as e:\n",
    "    print(\"Error: early_stopping_rounds is not supported in this version of LightGBM\")\n",
    "    print(\"Try using callbacks instead:\")\n",
    "    print(\"callbacks=[lgb.early_stopping(100)]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
